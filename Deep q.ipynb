{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = 1\n",
    "iterations = 20000\n",
    "max_memory = 1000000\n",
    "hidden_size = 30\n",
    "num_actions = 500\n",
    "input_size = 1\n",
    "batch_size = 128\n",
    "totalSteps = 0\n",
    "learningRate = 0.00025\n",
    "learnStart = 100\n",
    "updateTarget = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class environment():\n",
    "    def __init__(self, p, s):\n",
    "        self.p = p\n",
    "        self.s = s\n",
    "        self.time = 0\n",
    "        self.done = False\n",
    "        \n",
    "    def random_action(self):\n",
    "        return np.random.randint(1, self.s + 1)\n",
    "    \n",
    "    def step(self, amount):\n",
    "        if(np.random.rand() < self.p):\n",
    "            self.s += amount\n",
    "        else:\n",
    "            self.s -= amount\n",
    "        r = 0\n",
    "        done = False\n",
    "        if(self.s <= 0):\n",
    "            done = True\n",
    "            r = -1\n",
    "        if(self.s >= 500):\n",
    "            done = True\n",
    "            r = 1\n",
    "        self.time += 1\n",
    "        return self.s, r, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory, discount):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=batch_size):\n",
    "        indices = random.sample(np.arange(len(self.memory)), min(batch_size,len(self.memory)) )\n",
    "        miniBatch = []\n",
    "        for index in indices:\n",
    "            miniBatch.append(self.memory[index])\n",
    "        return miniBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "class DeepQ:\n",
    "    def __init__(self):\n",
    "        self.model = self.createModel('relu', learningRate)\n",
    "        self.target_model = self.createModel('relu', learningRate)\n",
    "\n",
    "    def createModel(self, activationType, learningRate):\n",
    "            layerSize = hidden_size\n",
    "            model = Sequential()\n",
    "            model.add(Dense(layerSize, input_shape=(input_size, ), init='lecun_uniform'))\n",
    "            model.add(Activation(activationType))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(layerSize, init='lecun_uniform'))\n",
    "            model.add(Activation(activationType))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(layerSize, init='lecun_uniform'))\n",
    "            model.add(Activation(activationType))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(layerSize, init='lecun_uniform'))\n",
    "            model.add(Activation(activationType))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(num_actions, init='uniform'))\n",
    "            model.add(Activation(\"softmax\"))\n",
    "            optimizer = optimizers.RMSprop(lr=learningRate, rho=0.9, epsilon=1e-06)\n",
    "            model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "            #print model.summary()\n",
    "            return model\n",
    "    def getAction(self, s):\n",
    "        state = np.array(s)\n",
    "        qValues = self.model.predict(state.reshape(1, 1))[0]\n",
    "        x = np.argmax(qValues) + 1 #outputs number from 1-500 inclusive\n",
    "        if x > s:\n",
    "            x = np.random.randint(1, s + 1)\n",
    "        return x\n",
    "    def updateTarget(self):\n",
    "        self.target_model = self.model\n",
    "    def trainModel(self, batch, discount):\n",
    "        X_batch = np.empty((0, input_size), dtype = np.float64)\n",
    "        Y_batch = np.empty((0, num_actions), dtype = np.float64)\n",
    "        for sample in batch:\n",
    "            state = np.array([sample[0][0]])\n",
    "            action = sample[0][1]\n",
    "            reward = sample[0][2]\n",
    "            newState = np.array([sample[0][3]])\n",
    "            isFinal = sample[1]\n",
    "            qValues = self.model.predict(state.reshape(1,len(state)))[0]\n",
    "            bestAction = np.argmax(self.target_model.predict(newState.reshape(1,len(newState)))[0])\n",
    "            qValuesNewState = self.model.predict(newState.reshape(1,len(newState)))[0]\n",
    "            targetValue = reward + discount * qValuesNewState[bestAction]\n",
    "\n",
    "            X_batch = np.append(X_batch, np.array([state.copy()]), axis=0)\n",
    "            Y_sample = qValues.copy()\n",
    "            Y_sample[action] = targetValue\n",
    "            Y_batch = np.append(Y_batch, np.array([Y_sample]), axis=0)\n",
    "            if isFinal:\n",
    "                X_batch = np.append(X_batch, np.array([newState.copy()]), axis=0)\n",
    "                Y_batch = np.append(Y_batch, np.array([[reward]*num_actions]), axis=0)\n",
    "        return self.model.train_on_batch(X_batch, Y_batch)\n",
    "    def save(self):\n",
    "        self.model.save_weights('my_model_weights.h5')\n",
    "    def load(self):\n",
    "        self.model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 1, 2, 1, 5, 2, 7, 9, 5, 8, 12, 4, 11, 3, 13, 2, 2, 15, 14, 18, 7, 2, 1, 22, 7, 5, 29, 7, 29, 19, 2, 1, 9, 20, 12, 12, 17, 26, 37, 24, 6, 23, 37, 24, 18, 19, 47, 18, 38, 37, 6, 48, 53, 55, 49, 7, 58, 41, 33, 43, 61, 15, 8, 31, 56, 26, 20, 38, 5, 28, 20, 67, 53, 69, 41, 7, 76, 3, 24, 51, 80, 55, 8, 41, 14, 64, 17, 79, 90, 87, 89, 84, 10, 90, 90, 12, 82, 95, 37, 11, 34, 7, 13, 12, 36, 93, 20, 68, 35, 56, 12, 76, 37, 9, 88, 80, 87, 15, 81, 119, 60, 118, 96, 108, 99, 60, 8, 20, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131]\n"
     ]
    }
   ],
   "source": [
    "DQN = DeepQ()\n",
    "strat = []\n",
    "for i in xrange(1, 500):\n",
    "    x = DQN.getAction(i)\n",
    "    strat.append(x)\n",
    "print strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Episode 9 finished after 8 timesteps with loss 0 and exploration value 0.913558883041: end=0\n",
      "1\n",
      "Episode 19 finished after 14 timesteps with loss 0.0831435394606 and exploration value 0.834589832783: end=0\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ExperienceReplay(max_memory, 0.99)\n",
    "DQN = DeepQ()\n",
    "history = []\n",
    "for i_episode in range(iterations):\n",
    "    #print i_episode, time / (i_episode+1)\n",
    "    env = environment(0.3, 200)\n",
    "    loss = 0\n",
    "    if e > 0.05:\n",
    "        e *= 0.991\n",
    "    while(env.done == False):\n",
    "        #env.render()\n",
    "        #print observation\n",
    "        s = env.s #get state\n",
    "        #Select action\n",
    "        if(np.random.rand() < e):\n",
    "            action = env.random_action()\n",
    "        else:\n",
    "            action = DQN.getAction(s)          \n",
    "        ss, r, done = env.step(action)\n",
    "        #remeber this state and action for later training\n",
    "        exp_replay.remember([s, action, r, ss], done)\n",
    "        #train model\n",
    "        s = ss\n",
    "        totalSteps += 1\n",
    "        if learnStart < totalSteps:\n",
    "            if totalSteps % updateTarget == 0:\n",
    "                DQN.updateTarget()\n",
    "            loss += DQN.trainModel(exp_replay.get_batch(batch_size), 0.99)\n",
    "        if done:\n",
    "            t = env.time\n",
    "            \n",
    "            if s >= 500:\n",
    "                history.append(1)\n",
    "            else:\n",
    "                history.append(0)\n",
    "            if(i_episode % 10 == 9):\n",
    "                print sum(history[len(history) - min(len(history), 100): len(history)])\n",
    "                print \"Episode {} finished after {} timesteps with loss {} and exploration value {}: end={}\".format(i_episode, t+1,\n",
    "                                                                                                    loss / (t + 1), e, s)\n",
    "            break\n",
    "#env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sum(history[len(history)-300:len(history)]) / 300.0\n",
    "DQN.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strat = []\n",
    "for i in xrange(1, 500):\n",
    "    x = DQN.getAction(i)\n",
    "    strat.append(x)\n",
    "print strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
