{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create neural net:\n",
    "Input: the amount of money (1-500)\n",
    "Output: the amount to bet (1-input)\n",
    "Objective is to maximize probability 200 will reach 500\n",
    "Can calculate this using linear equation solver\n",
    "p(i) = P * p(i+bet[i]) + (1 - P) * p(i-bet[i])\n",
    "P * p(i+bet[i]) + (1 - P) * p(i-bet[i]) - p(i) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = 1\n",
    "iterations = 20000\n",
    "max_memory = 100\n",
    "hidden_size = 30\n",
    "num_actions = 500\n",
    "input_size = 1\n",
    "batch_size = 128\n",
    "totalSteps = 0\n",
    "learningRate = 0.0025\n",
    "learnStart = 100\n",
    "updateTarget = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class environment():\n",
    "    def __init__(self, p, s):\n",
    "        self.p = p\n",
    "        self.s = s\n",
    "        self.time = 0\n",
    "        self.done = False\n",
    "        \n",
    "    def random_action(self):\n",
    "        return np.random.randint(1, self.s + 1)\n",
    "    \n",
    "    def probability(self, bets):\n",
    "        P = np.zeros(501) #0-500\n",
    "        p = self.p\n",
    "        a = []\n",
    "        P[500] = 1\n",
    "        first = np.zeros(501)\n",
    "        first[0] = 1\n",
    "        a.append(first)\n",
    "        for i in range(1, 500): #solving for P(i) for i = 1 to 500\n",
    "            t = np.zeros(501)\n",
    "            t[min(i + bets[i], 500)] = p\n",
    "            t[max(i - bets[i], 0)] = 1 - p\n",
    "            t[i] = -1\n",
    "            a.append(t)\n",
    "        last = np.zeros(501)\n",
    "        last[500] = 1\n",
    "        a.append(last)\n",
    "        a = np.array(a)\n",
    "        b = np.zeros(501)\n",
    "        b[500] = 1\n",
    "        x = np.linalg.solve(a, b)\n",
    "        return x\n",
    "        \n",
    "    def getBets(self, model):\n",
    "        strat = [0]\n",
    "        for i in xrange(1, 500):\n",
    "            x = model.getAction(i)\n",
    "            strat.append(x)\n",
    "        return strat\n",
    "    \n",
    "    def goodness(self, probs):\n",
    "        return sum(probs)\n",
    "    \n",
    "    def explore(self, bets, e):\n",
    "        for i in xrange(1, 500):\n",
    "            bets[i] += np.random.rand() * e * 15\n",
    "            bets[i] = min(i, bets[i])\n",
    "            bets[i] = max(1, bets[i])\n",
    "        return bets\n",
    "    \n",
    "    def step(self, amount, model, e):\n",
    "        bets = self.getBets(model)\n",
    "        r1 = self.probability(bets)        \n",
    "        bets[self.s] = amount\n",
    "        #bets = self.explore(bets, e)\n",
    "        r2 = self.probability(bets)\n",
    "        r = 0\n",
    "        if len(history) != 0:\n",
    "            r = self.goodness(r2) - sum(history[len(history) - min(len(history), 100): len(history)]) / min(100, len(history))\n",
    "        if(np.random.rand() < self.p):\n",
    "            self.s += amount\n",
    "        else:\n",
    "            self.s -= amount\n",
    "        done = False\n",
    "        if(self.s <= 0 or self.s >= 500):\n",
    "            done = True\n",
    "        self.s \n",
    "        self.time += 1\n",
    "        return self.s, r, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory, discount):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=batch_size):\n",
    "        indices = random.sample(np.arange(len(self.memory)), min(batch_size,len(self.memory)) )\n",
    "        miniBatch = []\n",
    "        for index in indices:\n",
    "            miniBatch.append(self.memory[index])\n",
    "        return miniBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "class DeepQ:\n",
    "    def __init__(self):\n",
    "        self.model = self.createModel('relu', learningRate)\n",
    "        self.target_model = self.createModel('relu', learningRate)\n",
    "\n",
    "    def createModel(self, activationType, learningRate):\n",
    "            layerSize = hidden_size\n",
    "            model = Sequential()\n",
    "            model.add(Dense(layerSize, input_shape=(input_size, ), init='lecun_uniform'))\n",
    "            model.add(Activation(activationType))\n",
    "            model.add(Dense(layerSize, init='lecun_uniform'))\n",
    "            model.add(Activation(activationType))\n",
    "            model.add(Dense(num_actions, init='lecun_uniform'))\n",
    "            model.add(Activation(\"softmax\"))\n",
    "            optimizer = optimizers.RMSprop(lr=learningRate, rho=0.9, epsilon=1e-06)\n",
    "            model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "            #print model.summary()\n",
    "            return model\n",
    "    def getAction(self, s):\n",
    "        state = np.array(s)\n",
    "        qValues = self.model.predict(state.reshape(1, 1))[0]\n",
    "        x = np.argmax(qValues) + 1 #outputs number from 1-500 inclusive\n",
    "        if x > s:\n",
    "            x = np.random.randint(1, s + 1)\n",
    "        return x\n",
    "    def updateTarget(self):\n",
    "        self.target_model = self.model\n",
    "    def trainModel(self, batch, discount):\n",
    "        X_batch = np.empty((0, input_size), dtype = np.float64)\n",
    "        Y_batch = np.empty((0, num_actions), dtype = np.float64)\n",
    "        for sample in batch:\n",
    "            state = np.array([sample[0][0]])\n",
    "            action = sample[0][1]\n",
    "            reward = sample[0][2]\n",
    "            newState = np.array([sample[0][3]])\n",
    "            isFinal = sample[1]\n",
    "            qValues = self.model.predict(state.reshape(1,len(state)))[0]\n",
    "            bestAction = np.argmax(self.target_model.predict(newState.reshape(1,len(newState)))[0])\n",
    "            qValuesNewState = self.model.predict(newState.reshape(1,len(newState)))[0]\n",
    "            targetValue = reward + discount * qValuesNewState[bestAction]\n",
    "\n",
    "            X_batch = np.append(X_batch, np.array([state.copy()]), axis=0)\n",
    "            Y_sample = qValues.copy()\n",
    "            Y_sample[action] = targetValue\n",
    "            Y_batch = np.append(Y_batch, np.array([Y_sample]), axis=0)\n",
    "            if isFinal:\n",
    "                X_batch = np.append(X_batch, np.array([newState.copy()]), axis=0)\n",
    "                Y_batch = np.append(Y_batch, np.array([[reward]*num_actions]), axis=0)\n",
    "        return self.model.train_on_batch(X_batch, Y_batch)\n",
    "    def save(self):\n",
    "        self.model.save_weights('my_model_weights.h5', overwrite=True)\n",
    "    def load(self):\n",
    "        self.model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 3, 1, 3, 8, 2, 10, 2, 2, 1, 13, 10, 16, 13, 5, 4, 3, 10, 13, 10, 12, 12, 23, 15, 13, 18, 13, 12, 16, 29, 9, 21, 13, 12, 2, 7, 30, 34, 7, 12, 10, 39, 17, 14, 39, 10, 36, 20, 49, 10, 53, 53, 28, 2, 47, 43, 38, 47, 59, 6, 31, 48, 50, 57, 13, 67, 28, 11, 40, 66, 53, 36, 50, 11, 34, 25, 55, 38, 25, 32, 64, 14, 59, 27, 40, 61, 1, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91]\n"
     ]
    }
   ],
   "source": [
    "DQN = DeepQ()\n",
    "strat = []\n",
    "for i in xrange(1, 500):\n",
    "    x = DQN.getAction(i)\n",
    "    strat.append(x)\n",
    "print strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409.300360348\n",
      "Episode 4 finished after 9 timesteps with goodness 81.842199151 and exploration value 0.955802742746: reward=0.248092010105\n",
      "819.555699031\n",
      "Episode 9 finished after 5 timesteps with goodness 81.8088083639 and exploration value 0.913558883041: reward=-0.149992216346\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ExperienceReplay(max_memory, 0.99)\n",
    "DQN = DeepQ()\n",
    "history = []\n",
    "for i_episode in range(iterations):\n",
    "    R = 0\n",
    "    #print i_episode, time / (i_episode+1)\n",
    "    env = environment(0.3, np.random.randint(1, 500))\n",
    "    loss = 0\n",
    "    if e > 0.05:\n",
    "        e *= 0.991\n",
    "    while(env.done == False):\n",
    "        #env.render()\n",
    "        #print observation\n",
    "        s = env.s #get state\n",
    "        #Select action\n",
    "        if(np.random.rand() < e):\n",
    "            action = env.random_action()\n",
    "        else:\n",
    "            action = DQN.getAction(s)          \n",
    "        ss, r, done = env.step(action, DQN, e)\n",
    "        R += r\n",
    "        #remeber this state and action for later training\n",
    "        exp_replay.remember([s, action, r, ss], done)\n",
    "        #train model\n",
    "        s = ss\n",
    "        totalSteps += 1\n",
    "        if learnStart < totalSteps:\n",
    "            if totalSteps % updateTarget == 0:\n",
    "                DQN.updateTarget()\n",
    "            loss += DQN.trainModel(exp_replay.get_batch(batch_size), 0.99)\n",
    "        if done:\n",
    "            t = env.time\n",
    "            history.append(env.goodness(env.probability(env.getBets(DQN))))\n",
    "            if(i_episode % 5 == 4):\n",
    "                DQN.save()\n",
    "                print sum(history[len(history) - min(len(history), 100): len(history)])\n",
    "                print \"Episode {} finished after {} timesteps with goodness {} and exploration value {}: reward={}\".format(i_episode, t+1,\n",
    "                                                                                        env.goodness(env.probability(env.getBets(DQN))), e, r)\n",
    "            break\n",
    "#env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sum(history[len(history)-300:len(history)]) / 300.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strat = []\n",
    "for i in xrange(1, 500):\n",
    "    x = DQN.getAction(i)\n",
    "    strat.append(x)\n",
    "print strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = environment(0.3, 200)\n",
    "bets = np.ones(501)\n",
    "print env.probability(0.3, bets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
